{
  "hidden_dim": 64,
  "num_transformer_layers": 1,
  "num_attention_heads": 4,
  "dropout": 0.23714372545797985,
  "lstm_num_layers": 2,
  "lstm_dropout": 0.11091554447065932,
  "learning_rate": 0.002690697764442766,
  "weight_decay": 0.0005988174485385435,
  "batch_size": 128,
  "weight_hidden_dim": 48,
  "weight_dropout": 0.29456574973419986,
  "num_epochs": 15
}